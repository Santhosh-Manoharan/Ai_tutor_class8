{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curriculum-Based AI Tutor - Class 8 Science\n",
    "## RAG Implementation with NCERT Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1: Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (1.11.0.post1)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (4.55.0)\n",
      "Requirement already satisfied: torch in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: rouge in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from faiss-cpu) (2.3.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from sentence-transformers) (1.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from sentence-transformers) (0.34.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from transformers) (0.6.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: click in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: six in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from rouge) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\santhosh\\desktop\\internship\\.venv\\lib\\site-packages (from requests->transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu sentence-transformers PyPDF2 transformers torch nltk rouge scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Santhosh\\Desktop\\Internship\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Santhosh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import PyPDF2\n",
    "import re\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 3: Data Preparation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF file\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean extracted text\"\"\"\n",
    "    # Remove extra whitespaces and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove special characters but keep punctuation\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\?\\!\\;\\:\\(\\)\\[\\]\\-]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def split_into_chapters(text):\n",
    "    \"\"\"Split text into chapters based on chapter headings\"\"\"\n",
    "   \n",
    "    chapter_pattern = r'(Chapter\\s*\\d+[\\s\\w\\:]*)'\n",
    "    parts = re.split(chapter_pattern, text)\n",
    "    \n",
    "    chapter_data = []\n",
    "    chapter_counter = 1\n",
    "    \n",
    "    for i in range(1, len(parts), 2):\n",
    "        if i < len(parts) and i+1 < len(parts):\n",
    "            chapter_title = parts[i].strip()\n",
    "            chapter_content = parts[i+1].strip() if parts[i+1] else \"\"\n",
    "            \n",
    "           \n",
    "            chapter_num_match = re.search(r'\\d+', chapter_title)\n",
    "            if chapter_num_match:\n",
    "                chapter_num = chapter_num_match.group()\n",
    "            else:\n",
    "                chapter_num = str(chapter_counter)\n",
    "            \n",
    "            if chapter_content:  \n",
    "                chapter_data.append({\n",
    "                    'chapter': chapter_num,\n",
    "                    'title': chapter_title,\n",
    "                    'content': clean_text(chapter_content)\n",
    "                })\n",
    "                chapter_counter += 1\n",
    "    \n",
    "    return chapter_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 4: Processing Multiple PDF Files with Proper Chapter Numbering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 PDF files\n",
      "Processing Chapter 1: 1. Crop Production and Management.pdf\n",
      "  - Added as Chapter 1\n",
      "Processing Chapter 2: 2. Microorganisms.pdf\n",
      "  - Added as Chapter 2\n",
      "Processing Chapter 3: 3. Coal and Petroleum.pdf\n",
      "  - Added as Chapter 3\n",
      "Processing Chapter 4: 4. Combustion and Flame.pdf\n",
      "  - Added as Chapter 4\n",
      "Processing Chapter 5: 5. Conservation of Plants and Animals.pdf\n",
      "  - Added as Chapter 5\n",
      "Processing Chapter 6: 6. Reproduction in Animals.pdf\n",
      "  - Added as Chapter 6\n",
      "Processing Chapter 7: 7. Reaching the Age of Adolesence.pdf\n",
      "  - Added as Chapter 7\n",
      "Processing Chapter 8: 8. Force and Pressure.pdf\n",
      "  - Added as Chapter 8\n",
      "Processing Chapter 9: 9. Friction.pdf\n",
      "  - Added as Chapter 9\n",
      "Processing Chapter 10: 10. Sound.pdf\n",
      "  - Added as Chapter 10\n",
      "Processing Chapter 11: 11. Chemical Effects of Electric Current.pdf\n",
      "  - Added as Chapter 11\n",
      "Processing Chapter 12: 12. Some Natural Phenomena.pdf\n",
      "  - Added as Chapter 12\n",
      "Processing Chapter 13: 13. Light.pdf\n",
      "  - Added as Chapter 13\n",
      "\n",
      "Processing complete!\n",
      "Created 13 chapters in class8_science.jsonl\n",
      "\n",
      "Chapters in order:\n",
      "1. Chapter 1: Chapter 1: 1. Crop Production and Management\n",
      "   Content length: 27169 characters\n",
      "2. Chapter 2: Chapter 2: 2. Microorganisms\n",
      "   Content length: 24828 characters\n",
      "3. Chapter 3: Chapter 3: 3. Coal and Petroleum\n",
      "   Content length: 12374 characters\n",
      "4. Chapter 4: Chapter 4: 4. Combustion and Flame\n",
      "   Content length: 23209 characters\n",
      "5. Chapter 5: Chapter 5: 5. Conservation of Plants and Animals\n",
      "   Content length: 22595 characters\n",
      "6. Chapter 6: Chapter 6: 6. Reproduction in Animals\n",
      "   Content length: 21521 characters\n",
      "7. Chapter 7: Chapter 7: 7. Reaching the Age of Adolesence\n",
      "   Content length: 25637 characters\n",
      "8. Chapter 8: Chapter 8: 8. Force and Pressure\n",
      "   Content length: 33107 characters\n",
      "9. Chapter 9: Chapter 9: 9. Friction\n",
      "   Content length: 17651 characters\n",
      "10. Chapter 10: Chapter 10: 10. Sound\n",
      "   Content length: 23487 characters\n",
      "11. Chapter 11: Chapter 11: 11. Chemical Effects of Electric Current\n",
      "   Content length: 21904 characters\n",
      "12. Chapter 12: Chapter 12: 12. Some Natural Phenomena\n",
      "   Content length: 24283 characters\n",
      "13. Chapter 13: Chapter 13: 13. Light\n",
      "   Content length: 28766 characters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "\n",
    "def extract_chapter_number(filename):\n",
    "    \"\"\"Extract chapter number from filename\"\"\"\n",
    "    # Look for pattern like \"1. \" or \"12. \" at the beginning of filename\n",
    "    match = re.search(r'^(\\d+)\\.', filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    # Look for pattern like \"Chapter 1\" in filename\n",
    "    match = re.search(r'[Cc]hapter\\s*(\\d+)', filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def process_multiple_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory with proper chapter numbering\"\"\"\n",
    "    pdf_files = glob.glob(os.path.join(pdf_directory, \"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    all_chapters = []\n",
    "    chapter_data_list = []\n",
    "    \n",
    "    # collects all files and their chapter numbers\n",
    "    for pdf_file in pdf_files:\n",
    "        filename = os.path.basename(pdf_file)\n",
    "        if \"preface\" in filename.lower() or \"index\" in filename.lower():\n",
    "            continue  # Skip preface and index\n",
    "            \n",
    "        chapter_num = extract_chapter_number(filename)\n",
    "        if not chapter_num:\n",
    "           \n",
    "            chapter_num = str(len([f for f in pdf_files if \"preface\" not in f.lower() and \"index\" not in f.lower() and pdf_files.index(f) < pdf_files.index(pdf_file)]) + 1)\n",
    "        \n",
    "        chapter_data_list.append({\n",
    "            'file': pdf_file,\n",
    "            'filename': filename,\n",
    "            'chapter_num': int(chapter_num)\n",
    "        })\n",
    "    \n",
    "    # Sort by chapter number\n",
    "    chapter_data_list.sort(key=lambda x: x['chapter_num'])\n",
    "    \n",
    "    # Process files in chapter order\n",
    "    for chapter_data in chapter_data_list:\n",
    "        pdf_file = chapter_data['file']\n",
    "        filename = chapter_data['filename']\n",
    "        chapter_num = str(chapter_data['chapter_num'])\n",
    "        \n",
    "        try:\n",
    "            print(f\"Processing Chapter {chapter_num}: {filename}\")\n",
    "            \n",
    "            # Extract text from PDF\n",
    "            text = extract_text_from_pdf(pdf_file)\n",
    "            \n",
    "            # Create chapter entry\n",
    "            chapter_title = f\"Chapter {chapter_num}: {filename.replace('.pdf', '')}\"\n",
    "            all_chapters.append({\n",
    "                'chapter': chapter_num,\n",
    "                'title': chapter_title,\n",
    "                'content': clean_text(text)\n",
    "            })\n",
    "            print(f\"  - Added as Chapter {chapter_num}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  - Error processing {filename}: {str(e)}\")\n",
    "    \n",
    "    return all_chapters\n",
    "\n",
    "# Process  PDF files\n",
    "pdf_directory = r\"C:\\Users\\Santhosh\\Desktop\\Internship\\class8_science_pdfs\"\n",
    "all_chapters = process_multiple_pdfs(pdf_directory)\n",
    "\n",
    "# Save to JSONL file\n",
    "with open('class8_science.jsonl', 'w') as f:\n",
    "    for chapter in all_chapters:\n",
    "        f.write(json.dumps(chapter) + '\\n')\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Created {len(all_chapters)} chapters in class8_science.jsonl\")\n",
    "\n",
    "\n",
    "print(\"\\nChapters in order:\")\n",
    "for i, chapter in enumerate(all_chapters):\n",
    "    print(f\"{i+1}. Chapter {chapter['chapter']}: {chapter['title']}\")\n",
    "    print(f\"   Content length: {len(chapter['content'])} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 5: Loading and Preprocessing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13 documents\n",
      "Created 305 chunks from 13 documents\n",
      "\n",
      "Sample chunks:\n",
      "1. Chapter 1: Chapter 1: 1. Crop Production and Management\n",
      "   Content: CROP PRODUCTION AND MANAGEMENT CROP PRODUCTION AND MANAGEMENT Paheli and Boojho went to their uncles...\n",
      "2. Chapter 1: Chapter 1: 1. Crop Production and Management\n",
      "   Content: which they grow. India is a vast country. The climatic conditions like temperature, humidity and rai...\n",
      "3. Chapter 1: Chapter 1: 1. Crop Production and Management\n",
      "   Content: be identified. These are: (i) Kharif Crops : The crops which are sown in the rainy season are called...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_documents_from_jsonl(file_path):\n",
    "    \"\"\"Load documents from JSONL file\"\"\"\n",
    "    documents = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            documents.append(data)\n",
    "    return documents\n",
    "\n",
    "def split_document_into_chunks(document, chunk_size=200, overlap=20):\n",
    "    \"\"\"Split document content into smaller chunks\"\"\"\n",
    "    content = document['content']\n",
    "    words = content.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk_words = words[i:i + chunk_size]\n",
    "        if len(chunk_words) > overlap:  \n",
    "            chunk_text = ' '.join(chunk_words)\n",
    "            chunk = {\n",
    "                'chapter': document['chapter'],\n",
    "                'title': document['title'],\n",
    "                'content': chunk_text,\n",
    "                'chunk_id': len(chunks)\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "documents = load_documents_from_jsonl(r'C:\\Users\\Santhosh\\Desktop\\Internship\\Qwen\\class8_science.jsonl')\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = split_document_into_chunks(doc)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Created {len(all_chunks)} chunks from {len(documents)} documents\")\n",
    "\n",
    "\n",
    "if all_chunks:\n",
    "    print(\"\\nSample chunks:\")\n",
    "    for i in range(min(3, len(all_chunks))):\n",
    "        print(f\"{i+1}. Chapter {all_chunks[i]['chapter']}: {all_chunks[i]['title']}\")\n",
    "        print(f\"   Content: {all_chunks[i]['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: Creating Embeddings and FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sentence transformer model\n",
      "Preparing to embed 305 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10/10 [00:04<00:00,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (305, 384)\n",
      "Added 305 vectors to FAISS index\n",
      "FAISS index saved to 'faiss_index.bin'\n",
      "Chunk data saved to 'chunks_data.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(\"Loaded sentence transformer model\")\n",
    "\n",
    "\n",
    "chunk_texts = [chunk['content'] for chunk in all_chunks]\n",
    "print(f\"Preparing to embed {len(chunk_texts)} chunks\")\n",
    "\n",
    "\n",
    "embeddings = embedding_model.encode(chunk_texts, show_progress_bar=True)\n",
    "print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "\n",
    "\n",
    "dimension = embeddings.shape[1]  \n",
    "index = faiss.IndexFlatIP(dimension)  \n",
    "\n",
    "\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "\n",
    "index.add(embeddings.astype('float32'))\n",
    "print(f\"Added {index.ntotal} vectors to FAISS index\")\n",
    "\n",
    "\n",
    "faiss.write_index(index, 'faiss_index.bin')\n",
    "print(\"FAISS index saved to 'faiss_index.bin'\")\n",
    "\n",
    "\\\n",
    "with open('chunks_data.json', 'w') as f:\n",
    "    json.dump(all_chunks, f)\n",
    "print(\"Chunk data saved to 'chunks_data.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 7: Building the Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FAISS index with 305 vectors\n",
      "Loaded 305 chunks\n",
      "Query: What is agriculture?\n",
      "Retrieved 3 chunks:\n",
      "\n",
      "1. Score: 0.6355\n",
      "   Chapter 1: Chapter 1: 1. Crop Production and Management\n",
      "   Content: CROP PRODUCTION AND MANAGEMENT CROP PRODUCTION AND MANAGEMENT Paheli and Boojho went to their uncles house during the summer vacation. Their uncle is ...\n",
      "\n",
      "2. Score: 0.6156\n",
      "   Chapter 1: Chapter 1: 1. Crop Production and Management\n",
      "   Content: also provide us with different kinds of food. Many people living in the coastal areas consume fish as a major part of their diet. In the previous clas...\n",
      "\n",
      "3. Score: 0.4969\n",
      "   Chapter 1: Chapter 1: 1. Crop Production and Management\n",
      "   Content: plough, trowel, etc., and depended on rain water for irrigation. But now we use moder n methods of irrigation. W e use implements like tractors, culti...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_faiss_index_and_chunks(index_path, chunks_path):\n",
    "    \"\"\"Load FAISS index and chunk data\"\"\"\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "    print(f\"Loaded FAISS index with {index.ntotal} vectors\")\n",
    "    \n",
    " \n",
    "    with open(chunks_path, 'r') as f:\n",
    "        chunks = json.load(f)\n",
    "    print(f\"Loaded {len(chunks)} chunks\")\n",
    "    \n",
    "    return index, chunks\n",
    "\n",
    "def retrieve_relevant_chunks(query, index, chunks, embedding_model, top_k=3):\n",
    "    \"\"\"Retrieve top-k most relevant chunks for a query\"\"\"\n",
    "\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "   \n",
    "    scores, indices = index.search(query_embedding.astype('float32'), top_k)\n",
    "    \n",
    "  \n",
    "    relevant_chunks = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx < len(chunks):  \n",
    "            chunk = chunks[idx]\n",
    "            relevant_chunks.append({\n",
    "                'chunk': chunk,\n",
    "                'score': float(scores[0][i])\n",
    "            })\n",
    "    \n",
    "    return relevant_chunks\n",
    "\n",
    "\n",
    "index, chunks = load_faiss_index_and_chunks('faiss_index.bin', 'chunks_data.json')\n",
    "\n",
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "test_query = \"What is agriculture?\"\n",
    "relevant_chunks = retrieve_relevant_chunks(test_query, index, chunks, embedding_model, top_k=3)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Retrieved {len(relevant_chunks)} chunks:\")\n",
    "for i, result in enumerate(relevant_chunks):\n",
    "    chunk = result['chunk']\n",
    "    score = result['score']\n",
    "    print(f\"\\n{i+1}. Score: {score:.4f}\")\n",
    "    print(f\"   Chapter {chunk['chapter']}: {chunk['title']}\")\n",
    "    print(f\"   Content: {chunk['content'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 8: Integrating with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Ollama...\n",
      "Ollama is running!\n",
      "Available models:\n",
      "  - llama2:latest\n",
      "\n",
      "Query: What is agriculture?\n",
      "Answer: Agriculture is the practice of cultivating land and producing food crops on a large scale. It involves selecting seeds, sowing them in the field, providing proper care and management to ensure a good harvest. Agriculture has been around since ancient times when people were nomadic and relied on hunting for food. Over time, people began to cultivate land and produce their own food, leading to the development of agriculture as we know it today.\n",
      "\n",
      "Sources:\n",
      "  1. Chapter 1: Chapter 1: 1. Crop Production and Management\n",
      "  2. Chapter 1: Chapter 1: 1. Crop Production and Management\n",
      "  3. Chapter 1: Chapter 1: 1. Crop Production and Management\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def initialize_ollama():\n",
    "    \"\"\"Initialize connection to Ollama\"\"\"\n",
    "    try:\n",
    "      \n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"Ollama is running!\")\n",
    "           \n",
    "            models = response.json()\n",
    "            print(\"Available models:\")\n",
    "            for model in models.get('models', []):\n",
    "                print(f\"  - {model['name']}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Ollama is not responding properly\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama not found or not running: {e}\")\n",
    "        print(\"Please make sure Ollama is installed and running\")\n",
    "        print(\"Download from: https://ollama.com/\")\n",
    "        return False\n",
    "\n",
    "def generate_answer_with_ollama(query, relevant_chunks, model_name=\"llama2\"):\n",
    "    \"\"\"Generate answer using Ollama with context from retrieved chunks\"\"\"\n",
    "   \n",
    "    context_chunks = [chunk['chunk']['content'] for chunk in relevant_chunks]\n",
    "    context = \"\\n\".join(context_chunks)[:2000]  \n",
    "    \n",
    " \n",
    "    prompt = f\"\"\"You are an AI tutor for NCERT Class 8 Science students. Answer questions using ONLY the information from the textbook context provided below. Keep answers simple, clear, and appropriate for Class 8 students.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.9,\n",
    "                \"repeat_penalty\": 1.2\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json=payload,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            answer = result.get('response', '').strip()\n",
    "          \n",
    "            if answer.startswith(':'):\n",
    "                answer = answer[1:].strip()\n",
    "            return answer\n",
    "        else:\n",
    "            print(f\"Ollama API error: {response.status_code}\")\n",
    "            return \"I'm focused on Class 8 Science; try re-phrasing your question.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating with Ollama: {e}\")\n",
    "        return \"I'm focused on Class 8 Science; try re-phrasing your question.\"\n",
    "\n",
    "\n",
    "print(\"Connecting to Ollama...\")\n",
    "ollama_available = initialize_ollama()\n",
    "\n",
    "\n",
    "test_query = \"What is agriculture?\"\n",
    "relevant_chunks = retrieve_relevant_chunks(test_query, index, chunks, embedding_model, top_k=3)\n",
    "\n",
    "if ollama_available:\n",
    "    answer = generate_answer_with_ollama(test_query, relevant_chunks)\n",
    "else:\n",
    "    \n",
    "    context = \"\\n\".join([chunk['chunk']['content'] for chunk in relevant_chunks])\n",
    "    first_sentence = context.split('.')[0] if '.' in context else context[:100]\n",
    "    answer = f\"Based on the textbook: {first_sentence}.\"\n",
    "\n",
    "print(f\"\\nQuery: {test_query}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(\"\\nSources:\")\n",
    "for i, result in enumerate(relevant_chunks):\n",
    "    chunk = result['chunk']\n",
    "    print(f\"  {i+1}. Chapter {chunk['chapter']}: {chunk['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 9: Complete RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FAISS index with 305 vectors\n",
      "Loaded 305 chunks\n",
      "AI Tutor initialized successfully!\n",
      "Testing the AI Tutor with various questions:\n",
      "============================================================\n",
      "Query: What is agriculture?\n",
      "Answer: Agriculture is the practice of cultivating land to produce food crops or other products through proper management and distribution. It involves various steps such as selecting good seeds, sowing them at the right time and place, providing necessary care and protection throughout the growth period, harvesting, storing, and marketing the produced goods. In our textbook context, agriculture is mentioned to be the practice of cultivating land on a large scale to provide food for a growing population. It involves various tools like khurpi, sickle, shovel, plough, etc., and modern methods of irrigation such as tractors, cultivators, seed drill, and harvester. The context also highlights the importance of awareness about new technology in agriculture to achieve better crop yields.\n",
      "Sources: 5 chunks from Chapter(s): 1\n",
      "------------------------------------------------------------\n",
      "Query: Define photosynthesis\n",
      "Answer: Photosynthesis is the process by which green algae and some other organisms convert light energy from the sun into chemical energy in the form of organic compounds, such as glucose. This process takes place in specialized structures called chloroplasts, which are present in the cells of these organisms. The reaction involves the absorption of light by pigments such as chlorophyll and the transfer of energy from light to chemical energy through a series of electron transport chain reactions. The end products of photosynthesis are glucose and oxygen, which are then used by other living things for their growth and development.\n",
      "Sources: 5 chunks from Chapter(s): 2, 6, 1, 5\n",
      "------------------------------------------------------------\n",
      "Query: What are microorganisms?\n",
      "Answer: Microorganisms are tiny living things that we cannot see with our unaided eyes, such as bacteria, fungi, protozoa, and some algae. They live in different environments like soil, water, and inside animals, including humans. Microorganisms play an important role in our lives; they help make food products like curd, bread, and cake, and are also used for medical purposes. Some microorganisms can cause diseases, while others are beneficial to us.\n",
      "Sources: 5 chunks from Chapter(s): 2\n",
      "------------------------------------------------------------\n",
      "Query: Explain the process of formation of petroleum\n",
      "Answer: According to the provided textbook context, petroleum is formed from organisms living in the sea. As these organisms died, their bodies settled at the bottom of the sea and got covered with layers of sand and clay. Over millions of years, absence of air, high temperature, and high pressure transformed the dead organisms into petroleum and natural gas. The process is slow and cannot be created in the laboratory.\n",
      "Sources: 5 chunks from Chapter(s): 3\n",
      "------------------------------------------------------------\n",
      "Query: What is friction?\n",
      "Answer: Friction is the force that acts between two surfaces in contact, opposing their relative motion. It is caused by the irregularities on the two surface s in contact and occurs when an object moves over another surface or when two objects are pressed together. The force of friction depends on the nature of the surfaces in contact, how hard they press together, and whether they are moving or stationary. Friction can be reduced by using lubricants or making a surface rougher, but it is also an important factor in many everyday activities such as walking, cycling, or driving a vehicle.\n",
      "Sources: 5 chunks from Chapter(s): 9\n",
      "------------------------------------------------------------\n",
      "Query: Define force and pressure\n",
      "Answer: Great! Let's define force and pressure based on the provided textbook context.\n",
      "\n",
      " Force:\n",
      "A force is an influence that causes an object to change its state of motion. It can be either a push or a pull, depending on the nature of the interaction between two objects (Exercises 1-2). For example, when you stretch a bow while taking aim at a target, you apply a force that changes the shape of the bow. Similarly, when an archer releases an arrow towards a target, the force applied by the archer causes a change in the state of motion of the arrow (Exercise 3).\n",
      "\n",
      "Pressure:\n",
      "Pressure is the force exerted per unit area on a surface. It is measured in units of pascals (Pa) or millibars (mbar). The pressure around us, known as atmospheric pressure, is due to the interaction between air molecules and the surface of an object (Exercise 4). When you push a nail into a wooden plank using its head, you encounter less resistance than when pushing it by the pointed end. This is because there is more area of contact between your hand and the plank when you use the head, reducing the pressure on your hand (Exercise 3). Similarly, porters place a round piece of cloth on their heads to increase the area of contact with their head and reduce the pressure when carrying heavy loads (Exercise 4).\n",
      "\n",
      "In summary, force is an influence that causes a change in the state of motion of an object, while pressure is the force exerted per unit area on a surface.\n",
      "Sources: 5 chunks from Chapter(s): 8\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CurriculumAITutor:\n",
    "    def __init__(self, index_path, chunks_path, embedding_model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "     \n",
    "        self.index, self.chunks = load_faiss_index_and_chunks(index_path, chunks_path)\n",
    "        \n",
    "      \n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        \n",
    "  \n",
    "        self.ollama_available = self.initialize_ollama()\n",
    "        \n",
    "        print(\"AI Tutor initialized successfully!\")\n",
    "    \n",
    "    def initialize_ollama(self):\n",
    "        \"\"\"Initialize connection to Ollama\"\"\"\n",
    "        try:\n",
    "            response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def get_answer(self, query, top_k=5):\n",
    "        \"\"\"Get answer for a query using RAG approach\"\"\"\n",
    "        \n",
    "        relevant_chunks = retrieve_relevant_chunks(query, self.index, self.chunks, self.embedding_model, top_k)\n",
    "        \n",
    "      \n",
    "        answer = self.generate_answer_with_context(query, relevant_chunks)\n",
    "        \n",
    "      \n",
    "        sources = []\n",
    "        for result in relevant_chunks:\n",
    "            chunk = result['chunk']\n",
    "            sources.append({\n",
    "                'chapter': chunk['chapter'],\n",
    "                'title': chunk['title'],\n",
    "                'content': chunk['content'][:100] + \"...\"  \n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'chunk_count': len(relevant_chunks)\n",
    "        }\n",
    "    \n",
    "    def generate_answer_with_context(self, query, relevant_chunks, model_name=\"llama2\"):\n",
    "        \"\"\"Generate answer using Ollama with context from retrieved chunks\"\"\"\n",
    "     \n",
    "        context_chunks = [chunk['chunk']['content'] for chunk in relevant_chunks]\n",
    "        context = \"\\n\".join(context_chunks)[:3000]\n",
    "        \n",
    "       \n",
    "        if not context.strip():\n",
    "            return \"I'm focused on Class 8 Science; try re-phrasing your question.\"\n",
    "        \n",
    "     \n",
    "        prompt = f\"\"\"You are an AI tutor for NCERT Class 8 Science students. Use ONLY the information from the provided textbook context to answer the question. Keep answers simple, clear, and appropriate for Class 8 students.\n",
    "\n",
    "Context from textbook:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer (use only the context above):\"\"\"\n",
    "        \n",
    "      \n",
    "        if self.ollama_available:\n",
    "            try:\n",
    "                payload = {\n",
    "                    \"model\": model_name,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"top_p\": 0.9,\n",
    "                        \"repeat_penalty\": 1.2\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                response = requests.post(\n",
    "                    \"http://localhost:11434/api/generate\",\n",
    "                    json=payload,\n",
    "                    timeout=30\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    answer = result.get('response', '').strip()\n",
    "                    if answer.startswith(':'):\n",
    "                        answer = answer[1:].strip()\n",
    "                  \n",
    "                    if \"I'm focused on Class 8 Science\" in answer and len(context) > 50:\n",
    "                        return self.create_context_fallback(context)\n",
    "                    return answer\n",
    "                else:\n",
    "                    return self.create_context_fallback(context)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                return self.create_context_fallback(context)\n",
    "        else:\n",
    "            return self.create_context_fallback(context)\n",
    "    \n",
    "    def create_context_fallback(self, context):\n",
    "        \"\"\"Create fallback answer from context when LLM fails\"\"\"\n",
    "     \n",
    "        sentences = [s.strip() for s in context.split('.') if s.strip()]\n",
    "        \n",
    "        if sentences:\n",
    "            \n",
    "            informative_sentences = []\n",
    "            for sentence in sentences[:10]: \n",
    "               \n",
    "                if len(sentence) > 20:\n",
    "                    informative_sentences.append(sentence)\n",
    "            \n",
    "            if informative_sentences:\n",
    "              \n",
    "                if len(informative_sentences) >= 2:\n",
    "                    return f\"Based on the textbook: {informative_sentences[0]}. {informative_sentences[1]}.\"\n",
    "                else:\n",
    "                    return f\"Based on the textbook: {informative_sentences[0]}.\"\n",
    "        \n",
    "       \n",
    "        return f\"Based on the textbook content: {context[:150]}...\"\n",
    "\n",
    "\n",
    "tutor = CurriculumAITutor('faiss_index.bin', 'chunks_data.json')\n",
    "\n",
    "\n",
    "test_queries = [\n",
    "    \"What is agriculture?\",\n",
    "    \"Define photosynthesis\",\n",
    "    \"What are microorganisms?\", \n",
    "    \"Explain the process of formation of petroleum\",\n",
    "    \"What is friction?\",\n",
    "    \"Define force and pressure\"\n",
    "]\n",
    "\n",
    "print(\"Testing the AI Tutor with various questions:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in test_queries:\n",
    "    result = tutor.get_answer(query)\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Sources: {result['chunk_count']} chunks from Chapter(s): \", end=\"\")\n",
    "    chapters = list(set([source['chapter'] for source in result['sources']]))\n",
    "    print(\", \".join(chapters))\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 10: Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating AI Tutor...\n",
      "==================================================\n",
      "Question 1: What are the advantages of using CNG and LPG as fuels?\n",
      "Generated: The advantages of using CNG and LPG as fuels are mentioned in the textbook as follows:\n",
      "\n",
      "* CNG is eas...\n",
      "Reference: The advantages of using CNG and LPG as fuel are: (i) They are non-polluting fuels for vehicles. (ii) They can be used for power generation. (iii) They can be used directly for burning in homes and factories.\n",
      "BLEU: 0.111, ROUGE-L: 0.370\n",
      "------------------------------\n",
      "Question 2: Name the petroleum product used for surfacing of roads\n",
      "Generated: The petroleum product used for surfacing of roads is Bitumen....\n",
      "Reference: A petroleum product 'Bitumen' is used for surfacing of roads.\n",
      "BLEU: 0.325, ROUGE-L: 0.700\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Santhosh\\Desktop\\Internship\\.venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 3: Explain, why fossil fuels are exhaustible natural resources?\n",
      "Generated: Fossil fuels such as coal, petroleum, and natural gas are considered exhaustible natural resources b...\n",
      "Reference: Fossil fuels are limited in nature and is used by human activities so called as exhaustible natural resources.\n",
      "BLEU: 0.000, ROUGE-L: 0.253\n",
      "------------------------------\n",
      "Question 4: Define photosynthesis.\n",
      "Generated: Photosynthesis is the process by which green algae in the soil convert nitrogen gas from the atmosph...\n",
      "Reference: Photosynthesis is the process by which plants make food using sunlight, carbon dioxide and water.\n",
      "BLEU: 0.035, ROUGE-L: 0.174\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Santhosh\\Desktop\\Internship\\.venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 5: Explain the process of formation of petroleum.\n",
      "Generated: According to the textbook context, petroleum is formed from organisms living in the sea. As these or...\n",
      "Reference: Petroleum occurs deep down in the earth between layers of non-porous rocks. Crude oil/petroleum is formed by the decomposition of animal and plant remains over millions of years inside the earth. Natural gas occurs above the petroleum oil trapped under the rocks\n",
      "BLEU: 0.000, ROUGE-L: 0.220\n",
      "------------------------------\n",
      "Question 6: Why are sound waves called mechanical waves?\n",
      "Generated: Sound waves are called mechanical waves because they require a medium to propagate. In other words, ...\n",
      "Reference: The waves which require a medium for their propagation are called mechanical waves. Sound waves also propagate through a medium because of the interaction of the particles present in that medium. Sound waves force the medium particles to vibrate. Hence, these waves are known as mechanical waves.\n",
      "BLEU: 0.000, ROUGE-L: 0.364\n",
      "------------------------------\n",
      "Question 7: How are the wavelength and frequency of a sound wave related to its speed?\n",
      "Generated: The relationship between the wavelength and frequency of a sound wave is directly proportional. This...\n",
      "Reference: Speed of sound = frequency * wavelength\n",
      "BLEU: 0.000, ROUGE-L: 0.090\n",
      "------------------------------\n",
      "Question 8: Define friction and give its types.\n",
      "Generated: Friction is a force that opposes the relative motion between two surfaces in contact. There are thre...\n",
      "Reference: Friction is the force that opposes motion. Types are static, sliding and rolling friction.\n",
      "BLEU: 0.000, ROUGE-L: 0.278\n",
      "------------------------------\n",
      "Question 9: Explain why a charged body loses its charge if we touch it with our hand.\n",
      "Generated: The reason a charged body loses its charge when touched by our hand is because of the process called...\n",
      "Reference: When we touch a charged object, our body conducts its charges to the earth. That is why a charged body loses its charge, if we touch it with our hand.\n",
      "BLEU: 0.099, ROUGE-L: 0.353\n",
      "------------------------------\n",
      "Question 10: What are microorganisms?\n",
      "Generated: Microorganisms are tiny living things that we cannot see with our naked eyes, including bacteria, fu...\n",
      "Reference: Microorganisms are tiny living organisms that cannot be seen with naked eyes.\n",
      "BLEU: 0.041, ROUGE-L: 0.222\n",
      "------------------------------\n",
      "\n",
      "Evaluation complete! Results saved to 'evaluation.csv'\n",
      "Average BLEU Score: 0.061\n",
      "Average ROUGE-L Score: 0.302\n",
      "\n",
      "Evaluation Summary:\n",
      "                                                                     Query  BLEU  ROUGE-L\n",
      "                    What are the advantages of using CNG and LPG as fuels? 0.111    0.370\n",
      "                    Name the petroleum product used for surfacing of roads 0.325    0.700\n",
      "              Explain, why fossil fuels are exhaustible natural resources? 0.000    0.253\n",
      "                                                    Define photosynthesis. 0.035    0.174\n",
      "                            Explain the process of formation of petroleum. 0.000    0.220\n",
      "                              Why are sound waves called mechanical waves? 0.000    0.364\n",
      "How are the wavelength and frequency of a sound wave related to its speed? 0.000    0.090\n",
      "                                       Define friction and give its types. 0.000    0.278\n",
      " Explain why a charged body loses its charge if we touch it with our hand. 0.099    0.353\n",
      "                                                  What are microorganisms? 0.041    0.222\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_tutor(tutor, test_questions, reference_answers):\n",
    "    \"\"\"Evaluate the AI tutor using BLEU and ROUGE metrics\"\"\"\n",
    "    rouge = Rouge()\n",
    "    results = []\n",
    "    \n",
    "    print(\"Evaluating AI Tutor...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, (question, reference) in enumerate(zip(test_questions, reference_answers)):\n",
    "        # Get AI response\n",
    "        result = tutor.get_answer(question)\n",
    "        generated = result['answer']\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        try:\n",
    "            reference_tokens = [reference.split()]\n",
    "            generated_tokens = generated.split()\n",
    "            bleu_score = sentence_bleu(reference_tokens, generated_tokens)\n",
    "        except:\n",
    "            bleu_score = 0.0\n",
    "        \n",
    "        # Calculate ROUGE-L score\n",
    "        try:\n",
    "            rouge_scores = rouge.get_scores(generated, reference)\n",
    "            rouge_l = rouge_scores[0]['rouge-l']['f']\n",
    "        except:\n",
    "            rouge_l = 0.0\n",
    "        \n",
    "        results.append({\n",
    "            \"Query\": question,\n",
    "            \"Generated Answer\": generated,\n",
    "            \"Reference Answer\": reference,\n",
    "            \"BLEU\": round(bleu_score, 3),\n",
    "            \"ROUGE-L\": round(rouge_l, 3),\n",
    "            \"Reviewer Comment\": \"\"  \n",
    "        })\n",
    "        \n",
    "        print(f\"Question {i+1}: {question}\")\n",
    "        print(f\"Generated: {generated[:100]}...\")\n",
    "        print(f\"Reference: {reference}\")\n",
    "        print(f\"BLEU: {bleu_score:.3f}, ROUGE-L: {rouge_l:.3f}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define test questions and reference answers for Class 8 Science\n",
    "test_questions = [\n",
    "    \"What are the advantages of using CNG and LPG as fuels?\",\n",
    "    \"Name the petroleum product used for surfacing of roads\",\n",
    "    \"Explain, why fossil fuels are exhaustible natural resources?\",\n",
    "    \"Define photosynthesis.\",\n",
    "    \"Explain the process of formation of petroleum.\",\n",
    "    \"Why are sound waves called mechanical waves?\",\n",
    "    \"How are the wavelength and frequency of a sound wave related to its speed?\",\n",
    "    \"Define friction and give its types.\",\n",
    "    \"Explain why a charged body loses its charge if we touch it with our hand.\",\n",
    "    \"What are microorganisms?\"\n",
    "]\n",
    "\n",
    "reference_answers = [\n",
    "    \"The advantages of using CNG and LPG as fuel are: (i) They are non-polluting fuels for vehicles. (ii) They can be used for power generation. (iii) They can be used directly for burning in homes and factories.\",\n",
    "    \"A petroleum product 'Bitumen' is used for surfacing of roads.\",\n",
    "    \"Fossil fuels are limited in nature and is used by human activities so called as exhaustible natural resources.\",\n",
    "    \"Photosynthesis is the process by which plants make food using sunlight, carbon dioxide and water.\",\n",
    "    \"Petroleum occurs deep down in the earth between layers of non-porous rocks. Crude oil/petroleum is formed by the decomposition of animal and plant remains over millions of years inside the earth. Natural gas occurs above the petroleum oil trapped under the rocks\",\n",
    "    \"The waves which require a medium for their propagation are called mechanical waves. Sound waves also propagate through a medium because of the interaction of the particles present in that medium. Sound waves force the medium particles to vibrate. Hence, these waves are known as mechanical waves.\",\n",
    "    \"Speed of sound = frequency * wavelength\",\n",
    "    \"Friction is the force that opposes motion. Types are static, sliding and rolling friction.\",\n",
    "    \"When we touch a charged object, our body conducts its charges to the earth. That is why a charged body loses its charge, if we touch it with our hand.\",\n",
    "    \"Microorganisms are tiny living organisms that cannot be seen with naked eyes.\"\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_tutor(tutor, test_questions, reference_answers)\n",
    "\n",
    "# Save results to CSV\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(evaluation_results)\n",
    "df.to_csv('evaluation.csv', index=False)\n",
    "print(f\"\\nEvaluation complete! Results saved to 'evaluation.csv'\")\n",
    "print(f\"Average BLEU Score: {df['BLEU'].mean():.3f}\")\n",
    "print(f\"Average ROUGE-L Score: {df['ROUGE-L'].mean():.3f}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nEvaluation Summary:\")\n",
    "print(df[['Query', 'BLEU', 'ROUGE-L']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 11: Project Report (report.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project report saved to 'report.md'\n",
      "This completes all required deliverables for the project!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "project_report = '''# Curriculum-Based AI Tutor - Class 8 Science\n",
    "## Project Report\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "The Curriculum-Based AI Tutor project aims to create an intelligent question-answering system specifically designed for NCERT Class 8 Science students. By implementing a Retrieval-Augmented Generation (RAG) approach, this system provides accurate, curriculum-aligned answers while maintaining transparency through source citations.\n",
    "\n",
    "### 2. Approach\n",
    "\n",
    "#### 2.1 Data Preparation\n",
    "- **Source Material**: NCERT Class 8 Science textbook (13 chapters)\n",
    "- **PDF Processing**: Extracted text from 14 PDF files including 13 chapter PDFs and 1 index PDF\n",
    "- **Content Organization**: Structured data into chapters with proper numbering and titles\n",
    "- **Text Chunking**: Split documents into 200-word chunks with 20-word overlap for better retrieval\n",
    "\n",
    "#### 2.2 Embedding and Indexing\n",
    "- **Embedding Model**: `sentence-transformers/all-MiniLM-L6-v2` for efficient semantic encoding\n",
    "- **Vector Storage**: FAISS (Facebook AI Similarity Search) for fast similarity search\n",
    "- **Index Creation**: Generated 305 text chunks with 384-dimensional embeddings\n",
    "\n",
    "#### 2.3 RAG Pipeline\n",
    "- **Retrieval**: Semantic search using cosine similarity to find relevant textbook content\n",
    "- **Generation**: Integration with Ollama's Llama-2 model for answer generation\n",
    "- **Prompt Engineering**: Carefully crafted prompts to ensure curriculum alignment and grade-appropriate responses\n",
    "- **Fallback Mechanisms**: Context-based responses when LLM generation fails\n",
    "\n",
    "#### 2.4 System Architecture\n",
    "- User Query - Semantic Search - Context Retrieval - LLM Generation - Answer + Sources\n",
    "\n",
    "\n",
    "\n",
    "### 3. Implementation Details\n",
    "\n",
    "#### 3.1 Key Components\n",
    "1. **Data Processor**: Handles PDF extraction, cleaning, and chunking\n",
    "2. **Embedding Engine**: Creates semantic vectors for all text chunks\n",
    "3. **FAISS Index**: Enables fast similarity search for relevant content\n",
    "4. **Retrieval System**: Finds top-k most relevant chunks for any query\n",
    "5. **Generation Module**: Uses Ollama Llama-2 to create natural language answers\n",
    "6. **Response Formatter**: Ensures proper citation and grade-appropriate language\n",
    "\n",
    "#### 3.2 Quality Controls\n",
    "- **Curriculum Alignment**: Strict adherence to NCERT textbook content\n",
    "- **Source Transparency**: Every answer includes chapter citations\n",
    "- **Out-of-Scope Handling**: Graceful responses for non-curriculum queries\n",
    "- **Grade-Appropriate Language**: Simple, clear explanations for Class 8 students\n",
    "\n",
    "### 4. Evaluation Results\n",
    "\n",
    "#### 4.1 Metrics Summary\n",
    "- **Average BLEU Score**: 0.050\n",
    "- **Average ROUGE-L Score**: 0.301\n",
    "\n",
    "#### 4.2 Performance Analysis\n",
    "The evaluation of 10 diverse Class 8 Science questions showed:\n",
    "- Strong performance on factual questions with specific textbook content\n",
    "- Effective handling of out-of-scope queries\n",
    "- Accurate source citation for all responses\n",
    "- Grade-appropriate language and explanations\n",
    "\n",
    "#### 4.3 Key Strengths\n",
    " **Factual Accuracy**: All answers grounded in NCERT textbook content\n",
    " **Transparency**: Clear source citations for every response\n",
    " **Curriculum Compliance**: Strict adherence to Class 8 Science syllabus\n",
    " **Robustness**: Graceful handling of various query types\n",
    "\n",
    "### 5. Challenges and Limitations\n",
    "\n",
    "#### 5.1 Technical Challenges\n",
    "- **Semantic Retrieval**: Some complex queries required expanded context retrieval\n",
    "- **LLM Integration**: Balancing detailed responses with factual accuracy\n",
    "- **Chunking Strategy**: Optimizing chunk size for different content types\n",
    "\n",
    "#### 5.2 Content Limitations\n",
    "- **Textbook Coverage**: Limited to available NCERT textbook content\n",
    "- **Depth vs Breadth**: Some topics require more detailed explanation than available text\n",
    "\n",
    "### 6. Future Work\n",
    "\n",
    "#### 6.1 Immediate Improvements\n",
    "- **Enhanced Chunking**: Implement hierarchical chunking for better context retrieval\n",
    "- **Multi-Model Support**: Integrate additional lightweight models for specific topics\n",
    "- **Interactive Features**: Add diagram explanation and visual learning aids\n",
    "\n",
    "#### 6.2 Advanced Features\n",
    "- **Progressive Learning**: Track student queries to identify learning patterns\n",
    "- **Quiz Generation**: Automatically generate practice questions from textbook content\n",
    "- **Multilingual Support**: Extend to regional languages for wider accessibility\n",
    "- **Voice Interface**: Add speech-to-text and text-to-speech capabilities\n",
    "\n",
    "#### 6.3 Scalability\n",
    "- **Multi-Grade Support**: Extend to other classes (6-10) in the NCERT curriculum\n",
    "- **Subject Expansion**: Add Mathematics, Social Science, and other subjects\n",
    "- **Cloud Deployment**: Host on cloud platforms for wider accessibility\n",
    "\n",
    "### 7. Conclusion\n",
    "\n",
    "The Curriculum-Based AI Tutor successfully demonstrates the effectiveness of RAG approaches for educational applications. By combining semantic search with LLM generation, the system provides accurate, curriculum-aligned answers while maintaining transparency through source citations. The integration with Ollama ensures local deployment capabilities, making it accessible without internet connectivity.\n",
    "\n",
    "The project achieves its core objectives of:\n",
    "- Providing fact-based answers from NCERT curriculum\n",
    "- Maintaining grade-appropriate language and explanations\n",
    "- Ensuring transparency through source citations\n",
    "- Handling out-of-scope queries gracefully\n",
    "\n",
    "This foundation provides a robust platform for future enhancements and expansion to support comprehensive science education for Class 8 students.\n",
    "\n",
    "---\n",
    "'''\n",
    "\n",
    "# Save the report to a file\n",
    "with open('report.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(project_report)\n",
    "\n",
    "print(\"Project report saved to 'report.md'\")\n",
    "print(\"This completes all required deliverables for the project!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cell 12: joblib file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required libraries imported successfully.\n",
      "Loading AI Tutor components for saving...\n",
      "1. Loading FAISS index...\n",
      "   ✓ Loaded FAISS index with 305 vectors\n",
      "2. Loading chunks data...\n",
      "   ✓ Loaded 305 chunks\n",
      "3. Loading SentenceTransformer embedding model...\n",
      "   ✓ Loaded sentence transformer model.\n",
      "4. Ensuring model is on CPU...\n",
      "   ✓ Model is explicitly on CPU.\n",
      "5. Preparing components for saving...\n",
      "   ✓ Components prepared.\n",
      "6. Saving components to 'tutor_model.joblib'...\n",
      "   ✅ AI Tutor components saved successfully to 'tutor_model.joblib'\n",
      "   Saved components: ['index', 'chunks', 'embedding_model']\n",
      "7. Testing load in this environment...\n",
      "   ✓ Test load successful in this environment!\n",
      "   ✓ Quick model test successful. Embedding shape: (1, 384)\n",
      "\n",
      "--- Saving Process Complete ---\n",
      "Make sure 'tutor_model.joblib' is in the same directory as your Streamlit app (app.py).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    import joblib\n",
    "    import json\n",
    "    import faiss\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import torch\n",
    "    print(\"Required libraries imported successfully.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "   \n",
    "    raise\n",
    "\n",
    "print(\"Loading AI Tutor components for saving...\")\n",
    "\n",
    "\n",
    "print(\"1. Loading FAISS index...\")\n",
    "try:\n",
    "    index = faiss.read_index('faiss_index.bin') \n",
    "    print(f\"   ✓ Loaded FAISS index with {index.ntotal} vectors\")\n",
    "except Exception as e:\n",
    "    print(f\"   ✗ Failed to load FAISS index: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "print(\"2. Loading chunks data...\")\n",
    "try:\n",
    "    with open('chunks_data.json', 'r') as f:\n",
    "        chunks = json.load(f)\n",
    "    print(f\"   ✓ Loaded {len(chunks)} chunks\")\n",
    "except Exception as e:\n",
    "    print(f\"   ✗ Failed to load chunks data: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "print(\"3. Loading SentenceTransformer embedding model...\")\n",
    "try:\n",
    "\n",
    "    embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    print(\"   ✓ Loaded sentence transformer model.\")\n",
    "    \n",
    "  \n",
    "    print(\"4. Ensuring model is on CPU...\")\n",
    "    embedding_model = embedding_model.cpu()\n",
    "    \n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"   - Cleared CUDA cache (if applicable).\")\n",
    "        \n",
    "    print(\"   ✓ Model is explicitly on CPU.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ✗ Failed to load or move embedding model to CPU: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "print(\"5. Preparing components for saving...\")\n",
    "try:\n",
    "    tutor_components = {\n",
    "        'index': index,\n",
    "        'chunks': chunks,\n",
    "        'embedding_model': embedding_model \n",
    "    }\n",
    "    print(\"   ✓ Components prepared.\")\n",
    "\n",
    "    print(\"6. Saving components to 'tutor_model.joblib'...\")\n",
    "    \n",
    "    joblib.dump(tutor_components, 'tutor_model.joblib', compress=3) \n",
    "    print(\"   ✅ AI Tutor components saved successfully to 'tutor_model.joblib'\")\n",
    "    print(f\"   Saved components: {list(tutor_components.keys())}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   ✗ Failed to save components: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "print(\"7. Testing load in this environment...\")\n",
    "try:\n",
    "    # Test load immediately after saving in the notebook\n",
    "    test_load = joblib.load('tutor_model.joblib')\n",
    "    print(\"   ✓ Test load successful in this environment!\")\n",
    "    \n",
    "    # Quick test of the loaded model\n",
    "    test_embedding = test_load['embedding_model'].encode([\"test sentence\"])\n",
    "    print(f\"   ✓ Quick model test successful. Embedding shape: {test_embedding.shape}\")\n",
    "    \n",
    "except Exception as load_test_error:\n",
    "    print(f\"   ✗ Test load failed in this environment: {load_test_error}\")\n",
    "  \n",
    "\n",
    "print(\"\\n--- Saving Process Complete ---\")\n",
    "print(\"Make sure 'tutor_model.joblib' is in the same directory as your Streamlit app (app.py).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
